---
permalink: /
title: ""
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

 
Hi! My name is Jing Xiong (熊璟), and I am currently a first-year PhD student at the University of Hong Kong, supervised by Prof. Yi Huang and Prof. Lingpeng Kong. I have published multiple papers in top-tier conferences and journals including ICLR, ICML, NeurIPS, ACL, EMNLP, and TMLR. My work primarily focuses on efficient language model inference and automated theorem proving.

Previously, I obtained my Master's degree from the School of Intelligent Systems Engineering, Sun Yat-sen University, under the supervision of [Prof. Chengming Li](https://ai.smbu.edu.cn/info/1251/1391.htm). Before that, I obtained my Bachelor's degree in Computer Science from Central South University in 2021. I had the honor of collaborating with [Prof. Xiaodan Liang](https://lemondan.github.io/).


#### My Interests
I am interested in research areas related to language modeling, efficient inference, automated theorem proving, and mathematical reasoning.


  
#### Service  
* Conference reviewer for: NAACL, EMNLP, ACL, ICML, ICLR, NIPS, COLING, etc.

#### Contact Information

Feel free to reach out to me via email at junexiong@connect.hku.hk or xiongj69@mail2.sysu.edu.cn if you have any questions or would like to collaborate!

News  
======
*05/2025*: Two paper are accepted in ACL 2025. 
*05/2025*: Two paper are accepted in ICML 2025.  
*03/2025*: [A paper](https://arxiv.org/abs/2404.02690) is accepted in SLLM Workshop at ICLR 2025. This work aims to bridge this gap by examining the inherent sparsity of standard attention processes.  
*03/2025*: [A paper](https://arxiv.org/abs/2411.05902) is accepted in TMLR.  
*01/2025*: Two paper are accepted in ICLR 2025.  
*09/2024*: Three paper are accepted in NIPS 2024.  
*01/2024*: [A paper](https://arxiv.org/abs/2310.00656) is accepted in ICLR 2024.  
*01/2024*: [A paper](https://arxiv.org/abs/2310.02954) is accepted in ICLR 2024.  
*11/2023*: The paper [TRIGO](https://arxiv.org/abs/2310.10180) has been selected for an oral presentation at EMNLP 2023.  
*10/2023*: [A paper](https://arxiv.org/abs/2310.10180) is accepted in EMNLP 2023.  
*9/2023*: [A paper](https://arxiv.org/pdf/2310.02954) has been submitted to ICLR 2024.  
*9/2023*: [A paper](https://arxiv.org/abs/2310.00656) has been submitted to ICLR 2024.  
*5/2023*: [A paper](https://aclanthology.org/2023.acl-long.706/) is accepted in ACL 2023.  
*5/2022*: [A paper](https://arxiv.org/abs/2310.15664) is accepted in SIGIR 2022.  


Publications
======
MEIT: Multi-Modal Electrocardiogram Instruction Tuning on Large Language Models for Report Generation  
ACL2025  
[arXiv](https://arxiv.org/abs/2403.04945), [Code](https://github.com/AIoT-MLSys-Lab/MEIT)

DAPE V2: Process Attention Score as Feature Map for Length Extrapolation  
ACL2025
<font color="grey">Chuanyang Zheng, Yihang Gao, Han Shi, <font color="black">Jing Xiong</font>, Jiankai Sun, Jingyao Li, Minbin Huang, Xiaozhe Ren, Michael Ng, Xin Jiang, Zhenguo Li, Yu Li</font>  
[arXiv](https://arxiv.org/abs/2410.04798), [Code](https://github.com/chuanyang-Zheng/DAPE)

ParallelComp: Parallel Long-Context Compressor for Length Extrapolation  
ICML2025  
<font color="grey"><font color="black">Jing Xiong</font>, Jianghan Shen, Chuanyang Zheng, Zhongwei Wan, Chenyang Zhao, Chiwun Yang, Fanghua Ye, Hongxia Yang, Lingpeng Kong, Ngai Wong</font>  
[arXiv](https://arxiv.org/abs/2502.14317),  [Code](https://github.com/menik1126/ParallelComp)

How Sparse Attention Approximates Exact Attention? Your Attention is Naturally n^C-Sparse  
SLLM  
<font color="grey">Yichuan Deng, Zhao Song, <font color="black">Jing Xiong</font>, Chiwun Yang</font>  
[arXiv](https://arxiv.org/abs/2404.02690)

Autoregressive Models in Vision: A Survey  
TMLR
<font color="grey"><font color="black">Jing Xiong</font>, Gongye Liu, Lun Huang, Chengyue Wu, Taiqiang Wu, Yao Mu, Yuan Yao, Hui Shen, Zhongwei Wan, Jinfa Huang, Chaofan Tao, Shen Yan, Huaxiu Yao, Lingpeng Kong, Hongxia Yang, Mi Zhang, Guillermo Sapiro, Jiebo Luo, Ping Luo, Ngai Wong</font>  
[arXiv](https://arxiv.org/abs/2411.05902),  [Code](https://github.com/ChaofanTao/Autoregressive-Models-in-Vision-Survey),  [Video](https://drive.google.com/file/d/1jEu0zf3jplhRXymnsb2eZy5avvSl868W/view)


FormalAlign: Automated Alignment Evaluation for Autoformalization

<font color="grey">Jianqiao Lu, Yingjia Wan, Yinya Huang, <font color="black">Jing Xiong</font>, Zhengying Liu, Zhijiang Guo</font>  
ICLR, 2025  
[arXiv](https://arxiv.org/abs/2410.10135),  [Code](https://github.com/rookie-joe/FormalAlign)  


D2O: Dynamic Discriminative Operations for Efficient Long-Context Inference of Large Language Models

<font color="grey">Zhongwei Wan, Xinjian Wu, Yu Zhang, Yi Xin, Chaofan Tao, Zhihong Zhu, Xin Wang, Siqi Luo, <font color="black">Jing Xiong</font>, Longyue Wang, Mi Zhang</font>  
ICLR, 2025  
[arXiv](https://arxiv.org/abs/2406.13035),  [Code](https://github.com/AIoT-MLSys-Lab/d2o)   


CAPE: Context-Adaptive Positional Encoding for Length Extrapolation

<font color="grey">Xun Wu, Shaohan Huang, Guolong Wang, <font color="black">Jing Xiong</font>, Furu Wei</font>  
Nips, 2024  
[arXiv](https://arxiv.org/html/2405.14722v1),  [Code](https://github.com/chuanyang-Zheng/DAPE)   




Multimodal large language models make text-to-image generative models align better

<font color="grey">Xun Wu, Shaohan Huang, Guolong Wang, <font color="black">Jing Xiong</font>, Furu Wei</font>  
Nips, 2024  
[arXiv](https://proceedings.neurips.cc/paper_files/paper/2024/hash/9421261e06f1a63a352b068f1ac90609-Abstract-Conference.html), [Code](https://github.com/yushuiwx/VisionPrefer.git)


Boosting text-to-video generative model with MLLMs feedback

<font color="grey">Xun Wu, Shaohan Huang, Guolong Wang, <font color="black">Jing Xiong</font>, Furu Wei</font>  
Nips, 2024  
[arXiv](https://proceedings.neurips.cc/paper_files/paper/2024/file/fbe2b2f74a2ece8070d8fb073717bda6-Paper-Conference.pdf)


LEGO-Prover: Neural Theorem Proving with Growing Libraries

<font color="grey">Huajian Xin, Haiming Wang, Chuanyang Zheng, Lin Li, Zhengying Liu, Qingxing Cao, Yinya Huang, <font color="black">Jing Xiong</font>, Han Shi, Enze Xie, Jian Yin, Zhenguo Li, Xiaodan Liang, Heng Liao</font>  
ICLR, 2024  
[arXiv](https://arxiv.org/abs/2310.00656), [Code](https://github.com/wiio12/LEGO-Prover)


DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for In-Context Learning  

<font color="grey"><font color="black">Jing Xiong</font>, Zixuan Li, Chuanyang Zheng, Zhijiang Guo, Yichun Yin, Enze Xie, Zhicheng Yang, Qingxing Cao, Haiming Wang, Xiongwei Han, Jing Tang, Chengming Li, Xiaodan Liang</font>  
ICLR, 2024  
[arXiv](https://arxiv.org/abs/2310.02954), [Code](https://github.com/AI4fun/DQ-LoRe)


TRIGO: Benchmarking Formal Mathematical Proof Reduction for Generative Language Models  

<font color="grey"><font color="black">Jing Xiong</font>, Jianhao Shen, Ye Yuan, Haiming Wang, Yichun Yin, Zhengying Liu, Lin Li, Zhijiang Guo, Qingxing Cao, Yinya Huang, Chuanyang Zheng, Xiaodan Liang, Ming Zhang, Qun Liu</font>
EMNLP, 2023  
[arXiv](https://arxiv.org/abs/2310.10180), [Code](https://github.com/menik1126/TRIGO), [Video](https://aclanthology.org/2023.emnlp-main.711.mp4)


DT-Solver: Automated Theorem Proving with Dynamic-Tree Sampling Guided by Proof-level Value Function
<font color="grey">Haiming Wang, Ye Yuan, Zhengying Liu, Jianhao Shen, Yichun Yin, <font color="black">Jing Xiong</font>, Enze Xie, Han Shi, Yujun Li, Lin Li, Jian Yin, Zhenguo Li, Xiaodan Liang</font>  
ACL, 2023  
[arXiv](https://aclanthology.org/2023.acl-long.706/)

Expression Syntax Information Bottleneck for Math Word Problems  
<font color="grey"><font color="black">Jing Xiong</font>, Chengming Li, Min Yang, Xiping Hu, Bin Hu</font>  
SIGIR, 2022  
[arXiv](https://dl.acm.org/doi/10.1145/3477495.3531824), [Code](https://github.com/menik1126/math_ESIB), [Video](https://dl.acm.org/doi/suppl/10.1145/3477495.3531824/suppl_file/SIGIR22-sp1591.mp4)


Self-consistent Reasoning For Solving Math Word Problems
<font color="grey"><font color="black">Jing Xiong</font>, Zhongwei Wan, Xiping Hu, Min Yang, Chengming Li</font>  
[arXiv](https://arxiv.org/abs/2210.15373), [Code](https://github.com/menik1126/math_SCL) 


CV
======
Please check my [resume](Jing_Xiong_s_CV_10_25.pdf) here.
